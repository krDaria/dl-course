{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of nmt-2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krDaria/dl-course/blob/master/nmt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQKHyD6k89r",
        "colab_type": "text"
      },
      "source": [
        "[Open With Colab](https://colab.research.google.com/github/m12sl/dl_cshse_2019/blob/master/seminars/x2seq/nmt.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0hUc8aEg8S2_",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Au270_eo8T7b",
        "outputId": "ad2559e0-a75e-4a10-96fe-9c6623db90e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "! wget https://github.com/m12sl/dl_cshse_2019/raw/master/seminars/x2seq/eng-rus.tar.gz\n",
        "! tar xzvf eng-rus.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-11 17:37:01--  https://github.com/m12sl/dl_cshse_2019/raw/master/seminars/x2seq/eng-rus.tar.gz\n",
            "Resolving github.com (github.com)... 13.250.177.223\n",
            "Connecting to github.com (github.com)|13.250.177.223|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/m12sl/dl_cshse_2019/master/seminars/x2seq/eng-rus.tar.gz [following]\n",
            "--2019-06-11 17:37:02--  https://raw.githubusercontent.com/m12sl/dl_cshse_2019/master/seminars/x2seq/eng-rus.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7020408 (6.7M) [application/octet-stream]\n",
            "Saving to: ‘eng-rus.tar.gz.2’\n",
            "\n",
            "eng-rus.tar.gz.2    100%[===================>]   6.69M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-06-11 17:37:03 (92.3 MB/s) - ‘eng-rus.tar.gz.2’ saved [7020408/7020408]\n",
            "\n",
            "eng-rus.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qdM5tBYV8S3J"
      },
      "source": [
        "## Наивный вариант представления текстов\n",
        "\n",
        "0. Нормализовать написание\n",
        "1. Отфильтруем все спецсимволы\n",
        "2. Разобьем по пробелам, сделаем *наивную токенизацию*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qSYbvPOW8S3L",
        "outputId": "59f162b9-8b10-4999-e821-c5fec977bcab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# Приготовим данные и посмотрим на них\n",
        "# Кроме словаря нас интересует еще набор символов\n",
        "raw_alphabet = set()\n",
        "alphabet = set()\n",
        "def normalize(s):\n",
        "    return \"\".join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess(s):\n",
        "    raw_alphabet.update(s)\n",
        "    s = normalize(s.lower().strip())\n",
        "    s = re.sub(r\"[^a-zа-я?.,!]+\", \" \", s)\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    alphabet.update(s)\n",
        "    return s\n",
        "\n",
        "pairs = []\n",
        "with open('eng-rus.txt', 'r') as fin:\n",
        "    for line in tqdm(fin.readlines()):\n",
        "        pair = [preprocess(_) for _ in line.split('\\t')]\n",
        "        pairs.append(pair)\n",
        "        \n",
        "print(\"RAW alphabet {} symbols:\".format(len(raw_alphabet)), \n",
        "      \"\".join(sorted(raw_alphabet)))\n",
        "print(\"After preprocessing {} symbols: \".format(len(alphabet)), \n",
        "      \"\".join(sorted(alphabet)))\n",
        "print(\"There are {} pairs\".format(len(pairs)))\n",
        "print(pairs[10000])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bf154a3fe364665aeb7d7cf65df38a3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=336666), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RAW alphabet 174 symbols: \n",
            " !\"$%&'()+,-./0123456789:;?@ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz «°º»ãçéêîïóöúǘЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёׁ​–—―‘’… ‽₂€№\n",
            "After preprocessing 62 symbols:   !,.?abcdefghijklmnopqrstuvwxyzабвгдежзиклмнопрстуфхцчшщъыьэюя\n",
            "There are 336666 pairs\n",
            "['it s too fast .', 'это слишком быстро .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h9ARXr6b8S3U"
      },
      "source": [
        "Каждому слову поставим в соответсвие номер + нам потребуются спецтокены для начала и конца последовательности и для неизвестных слов.\n",
        "`<SOS>, <EOS>, <UNK>`\n",
        "\n",
        "У нас два языка, для работы с каждым нам потребуются словами и функции для перевода из слов в номера и обратно.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2epLOt_-8S3V",
        "outputId": "c51b8b0d-d12e-40d3-d5a5-24cca9009895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "COMMON_TOKENS = ['PAD', 'SOS', 'EOS', 'UNK']\n",
        "\n",
        "\n",
        "def build_vocabs(sents, max_size=1000):\n",
        "    cnt = Counter()\n",
        "    for s in sents:\n",
        "        cnt.update(s.split(' '))\n",
        "        \n",
        "    print('There are {} tokens'.format(len(cnt)))\n",
        "    toks = COMMON_TOKENS + [_[0] for _ in cnt.most_common(max_size - len(COMMON_TOKENS))]\n",
        "    tok2idx = {t: i for i, t in enumerate(toks)}\n",
        "    idx2tok = {i: t for t, i in tok2idx.items()}\n",
        "    print('Truncate to {} toks'.format(len(tok2idx)))\n",
        "    return tok2idx, idx2tok\n",
        "\n",
        "\n",
        "eng, rus = list(zip(*pairs))\n",
        "rus2idx, idx2rus = build_vocabs(rus, max_size=10000)\n",
        "eng2idx, idx2eng = build_vocabs(eng, max_size=5000)\n",
        "\n",
        "def sentence2idx(s, tok2idx):\n",
        "    tokens = preprocess(s).split(' ')\n",
        "    unk = tok2idx['UNK']\n",
        "    return [tok2idx['SOS']] + [tok2idx.get(_, unk) for _ in tokens] + [tok2idx['EOS']]\n",
        "\n",
        "\n",
        "def idx2sentence(s, idx2tok):\n",
        "    return \" \".join(idx2tok[_] for _ in s)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 57309 tokens\n",
            "Truncate to 10000 toks\n",
            "There are 17459 tokens\n",
            "Truncate to 5000 toks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xh5koecS8S3c",
        "outputId": "fdcc46a9-8195-4aca-833a-2cfa11e5557a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# проверим консистентность преобразований\n",
        "x = sentence2idx('Привет мир!', rus2idx)\n",
        "print(x)\n",
        "print(idx2sentence(x, idx2rus))\n",
        "\n",
        "x = sentence2idx('Hello world!', eng2idx)\n",
        "print(x)\n",
        "print(idx2sentence(x, idx2eng))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2540, 1265, 83, 2]\n",
            "SOS привет мир ! EOS\n",
            "[1, 1961, 440, 175, 2]\n",
            "SOS hello world ! EOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dhjjx52i8S3g"
      },
      "source": [
        "## Работа с последовательностями произвольной длинны в pytorch\n",
        "\n",
        "Нам нужно уметь генерировать батчи тензоров `[bs, 1, seq_len]`.\n",
        "Но в нашем датасете семплы разной длины:\n",
        "\n",
        "- мы могли бы подрезать все до минимальной\n",
        "- паддить до максимальной\n",
        "- выбрать какую-то среднюю длину"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmGX7wtL8S3i",
        "outputId": "0e60152d-dd8b-4e33-f3c5-022757fe590f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# сделаем датасет с закодированными парами:\n",
        "class EngRusDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        eng, rus = self.pairs[item]\n",
        "        return dict(\n",
        "            eng=eng,\n",
        "            rus=rus,\n",
        "        )\n",
        "\n",
        "encoded = []\n",
        "for eng, rus in tqdm(pairs):\n",
        "    a = sentence2idx(eng, eng2idx)\n",
        "    b = sentence2idx(rus, rus2idx)\n",
        "    encoded.append((a, b))\n",
        "    \n",
        "ds = EngRusDataset(encoded)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1f2c1b72b804884b9d15de72fe1bca2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=336666), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9PDpAtEq8S3n"
      },
      "source": [
        "Давайте соберем наивный DataLoader и посмотрим как он делает батчи:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-wLIzt88S3o",
        "colab": {}
      },
      "source": [
        "dataloader = DataLoader(ds, batch_size=8, shuffle=True)\n",
        "it = iter(dataloader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CM0vzGL8S3s",
        "colab": {}
      },
      "source": [
        "next(it)['eng']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O7blZtcq8S3w"
      },
      "source": [
        "В моем случае, результат запуска был таков:\n",
        "```\n",
        "[tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
        " tensor([ 6,  7,  6, 15,  5,  6,  5, 62]),\n",
        " tensor([ 48,  34,  83,   7,  32, 221,  22,  43]),\n",
        " tensor([  5, 143,  37,  36, 129,  12,  11,  66]),\n",
        " tensor([  73, 1258,  279,    8,    6,  555,   41,   10]),\n",
        " tensor([  8, 140,   8, 628,  20,  96,  13, 270]),\n",
        " tensor([  47,    4,   15,   18,   55,  269,    6, 1287]),\n",
        " tensor([ 58,   2,  13, 140, 193, 140, 171, 140])]\n",
        "```\n",
        "\n",
        "Какие странности здесь видны?\n",
        "1. Это не тензор, а список тензоров\n",
        "2. На `<EOS>` (2) оканчивается только один пример, остальные подрезаны под его длину.\n",
        "\n",
        "Мы бы хотели западдить все примеры до длины максимального в батче. \n",
        "Но на этапе подготовки семпла мы не знаем соседей по батчу!\n",
        "Нам пригодиться параметр collate_fn в конструкторе DataLoader:\n",
        "\n",
        "```\n",
        "def collate_fn(samples):\n",
        "    # samples -- список семплов-словарей\n",
        "    <...>\n",
        "    return batch\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SMSeFFHQ8S3y",
        "colab": {}
      },
      "source": [
        "def collate_fn(samples):\n",
        "    PAD = 0\n",
        "    def _pad_to_longest(lst, pad_left=False):\n",
        "        longest = max(len(s) for s in lst)\n",
        "        if pad_left:\n",
        "            return torch.LongTensor([[PAD] * (longest - len(s)) + s for s in lst])\n",
        "        else:\n",
        "            return torch.LongTensor([s + [PAD] * (longest - len(s)) for s in lst])\n",
        "        \n",
        "    eng = [s['eng'] for s in samples]\n",
        "    rus = [s['rus'] for s in samples]\n",
        "    \n",
        "    return dict(\n",
        "        rus=_pad_to_longest(rus, pad_left=False),\n",
        "        eng=_pad_to_longest(eng, pad_left=True),\n",
        "    )\n",
        "\n",
        "dataloader = DataLoader(ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "it = iter(dataloader)\n",
        "next(it)['eng']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dph7rI9_8S33",
        "colab": {}
      },
      "source": [
        "# Теперь напишем модельку, которая может переводить!\n",
        "# Соберем модель из двух частей:\n",
        "# - Encoder на RNN\n",
        "# - Decoder на RNN\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, layers=1):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=layers)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embeddings(input)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size=1, device=None):\n",
        "        # be aware about dimension! https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "enc = EncoderRNN(256, len(eng2idx))\n",
        "x = next(it)['eng']\n",
        "print(x.shape)\n",
        "hidden = enc.init_hidden(8)\n",
        "out, hidden = enc(x, hidden)\n",
        "print(out.shape, hidden.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oYfecQb88S38",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, layers=1):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=layers)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embeddings(input)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden\n",
        "        \n",
        "    def init_hidden(self, batch_size=1, device=None):\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)\n",
        "    \n",
        "# декодер получит тензор с закодированным состоянием и батч первых токенов последовательности для генерации"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oyMwF94n8S3_",
        "colab": {}
      },
      "source": [
        "y = next(it)['rus']\n",
        "dec = DecoderRNN(256, len(rus2idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVFcg0Uz8S4C",
        "colab": {}
      },
      "source": [
        "# проверяем размерности подаваемых и возвращаемых тензоров на каждом шаге\n",
        "for i in range(0, y.shape[1]):\n",
        "    t = y[:, i].view(-1, 1)\n",
        "    print(t.shape)\n",
        "    o, z = dec(t, hidden)\n",
        "    print(o.shape, z.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZiWyjUx8S4N",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ThQj4uOx8S4P",
        "colab": {}
      },
      "source": [
        "device = \"cuda\"\n",
        "encoder = EncoderRNN(256, len(eng2idx)).to(device)\n",
        "decoder = DecoderRNN(256, len(rus2idx)).to(device)\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-2)\n",
        "dataloader = DataLoader(ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "log = []\n",
        "def train_model(dataloader, optimizer, teacher_forcing=True):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    for batch in tqdm(dataloader):\n",
        "        eng = batch['eng'].to(device)\n",
        "        rus = batch['rus'].to(device)\n",
        "        encoder_hidden = encoder.init_hidden(eng.size(0)).to(device)\n",
        "        encoder_outputs, hidden = encoder(eng, encoder_hidden)\n",
        "        \n",
        "        # мы добавляли <SOS> во все последовательности, так что предсказывать будем начиная со второй позиции\n",
        "        loss = 0.0\n",
        "        weight = 0.0\n",
        "        x = rus[:, 0].view(-1, 1)\n",
        "        for i in range(1, rus.size(1)):\n",
        "            out, hidden = decoder(x, hidden)\n",
        "            \n",
        "            target = rus[:, i].view(-1, 1)\n",
        "            # маскируем все паддинги\n",
        "            mask = 1.0 * (target > 0)\n",
        "            weight += 1.0 * mask.sum()\n",
        "            loss += torch.sum(F.nll_loss(out.squeeze(1), target.squeeze(1)) * mask.float())\n",
        "            \n",
        "            if teacher_forcing:\n",
        "                x = rus[:, i].view(-1, 1)\n",
        "            else:\n",
        "                # здесь могло бы быть семплирование из вероятностей\n",
        "                _, topi = out.topk(1, dim=-1)\n",
        "                x = topi.squeeze(-1).detach()\n",
        "        loss /= (weight.float() + 1e-5)\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        log.append(loss.item())\n",
        "\n",
        "train_model(dataloader, optimizer, teacher_forcing=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lrN-alp88S4X",
        "colab": {}
      },
      "source": [
        "plt.plot(log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "64h3_w528S4d",
        "colab": {}
      },
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "encoder = encoder.to(\"cpu\")\n",
        "decoder = decoder.to(\"cpu\")\n",
        "\n",
        "def evaluate(sentence, T=1.0):\n",
        "    encoded = sentence2idx(sentence, eng2idx)\n",
        "    output = []\n",
        "    print(encoded)\n",
        "    bs = 10\n",
        "    with torch.no_grad():\n",
        "      \n",
        "        z = torch.LongTensor(encoded).view(1, -1).repeat(bs, 1)\n",
        "        encoder_outputs, hidden = encoder(z, encoder.init_hidden(bs))\n",
        "        x = torch.LongTensor([1]).view(1, 1).repeat(bs, 1)\n",
        "        for i in range(20):\n",
        "            out, hidden = decoder(x, hidden)\n",
        "            x = torch.multinomial(F.softmax(out / T, dim=-1).squeeze(1), 1)\n",
        "#             _, topi = out.topk(1, dim=-1)\n",
        "#             x = topi.squeeze(-1).detach()\n",
        "            tokens = x.squeeze(1).cpu().numpy()\n",
        "            output.append(tokens)\n",
        "    \n",
        "    output = np.array(output).T\n",
        "    for s in output:\n",
        "        out = idx2sentence(s, idx2rus)\n",
        "        print(out.replace('PAD', \"\"))\n",
        "\n",
        "    \n",
        "evaluate(\"What is going on?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AxG4rpnB8S4l",
        "colab": {}
      },
      "source": [
        "Варианты простых улучшений:\n",
        "1. Attention over encoder outputs (try decomposable attention)\n",
        "2. seq2seq + Autoencoder, с возможностью перевода lang->state->lang\n",
        "3. BPE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8UoLUjTo8S4t",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "raw",
        "id": "2jfWcdjw8S4x"
      },
      "source": [
        ""
      ]
    }
  ]
}